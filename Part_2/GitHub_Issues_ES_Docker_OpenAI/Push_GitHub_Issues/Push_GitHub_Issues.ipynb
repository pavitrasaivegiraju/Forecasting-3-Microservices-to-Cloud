{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c8a7cc-de53-4725-8ae6-8c128e581b8b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    \n",
    "### <center> GITHUB ISSUES</center>\n",
    "### <center> ELASTICSEARCH - OPEN AI</center>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "<br>\n",
    "    <br>\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffddc3a-6993-4848-bea0-6e392ae30da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\pavit\\appdata\\roaming\\python\\python311\\site-packages (1.16.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pavit\\appdata\\roaming\\python\\python311\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.10.12)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pavit\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pavit\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c32c3-fdb5-4391-800f-38864eaaae01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8da7f1d-cc25-452e-9aa1-f019edfd90c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: elasticsearch in c:\\users\\pavit\\appdata\\roaming\\python\\python311\\site-packages (8.13.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.13 in c:\\users\\pavit\\appdata\\roaming\\python\\python311\\site-packages (from elasticsearch) (8.13.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.0.7)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "#Install elastic search\n",
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df79588c-116b-4bc7-86df-e6ce16ce6f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7aee99d-0219-4469-9f8a-d1c32463ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import requests\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d15e9-bc44-4510-b3c5-40632ebc8299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13b0610-c0c9-4e04-9068-d332231b904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the headers\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"access_token\": \"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\",\n",
    "    \"Git_Username\":\"PSP24SCM04V\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f3b1c-0863-40a0-b3e0-40e7b26ce0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efce5822-6d66-49b7-9523-ba0c3a074da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the owner and the repository\n",
    "owners = ['openai', 'elastic', 'openai', 'milvus-io', 'SebastianM']\n",
    "repos = ['openai-cookbook','elasticsearch', 'openai-python', 'pymilvus', 'angular-google-maps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f4a9c-94f6-43d5-8d64-e5521cea24d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "decd06a3-5c46-4048-a22b-180bde820141",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "per_page = 100\n",
    "from_date = (dt.date.today() - dt.timedelta(days=60)).isoformat() #The duration for which we need the issues can be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116343df-7e18-4c8b-b285-f4bf00534000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c0bc88-58d5-43ff-ad3c-9a8d3cb71e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that returns the base url\n",
    "def fetch_url(owner, repo):\n",
    "    return f\"https://\"+headers[\"Git_Username\"]+\":\"+headers[\"access_token\"]+f\"@api.github.com/repos/{owner}/{repo}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ace28-fedd-461a-92f8-76b870e673af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515dc9c3-429f-4172-a3a9-0d9ba5e96752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the Issues from the GitHub repository\n",
    "issues=[]\n",
    "for owner in owners:\n",
    "    for repo in repos:\n",
    "        if (owner=='openai' and repo=='openai-python') or (owner=='openai' and repo=='openai-cookbook') or (owner=='elastic' and repo=='elasticsearch') or (owner=='milvus-io' and repo=='pymilvus') or (owner=='SebastianM' and repo=='angular-google-maps'):\n",
    "            flag = True\n",
    "            url = fetch_url(owner, repo)\n",
    "            while flag:\n",
    "                response = requests.get(f\"{url}/issues\", headers=headers,params={\"since\": from_date, \"page\": page,\"state\":\"all\"})\n",
    "                for obj in response.json():\n",
    "                    if datetime.strptime(from_date, \"%Y-%m-%d\") <= datetime.strptime(obj[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\"):\n",
    "                        issueObject = {\n",
    "                        \"_type\": \"issue\",\n",
    "                        \"_repo\":repo,    \n",
    "                        \"_issueNumber\": str(obj['number']),\n",
    "                        \"_title\": str(obj['title']),\n",
    "                        \"_createdAt\": str(obj['created_at']),\n",
    "                        \"_closedAt\": str(obj['closed_at']) if str(obj['closed_at']) != \"None\" else \"2024-12-31T00:36:30Z\", # Few Issues might still be open, we add \"2024-12-31T00:36:30Z\" as closed date for those Issues.\n",
    "                        \"_state\": str(obj['state']),\n",
    "                        \"_body\": str(obj['body'])[:5000] \n",
    "                            # Here we are considering only the first 5000 characters from the body as \n",
    "                            # there is a limit on the the text tokens that we can embed using the openai model.\n",
    "            # Please refer https://platform.openai.com/docs/guides/embeddings to know more about the embedding models. \n",
    "    \n",
    "            # Please refer to https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb to see how tokens are counted.\n",
    "                    \n",
    "                        }\n",
    "                        issues.append(issueObject)                      \n",
    "                    else:\n",
    "                        flag = False\n",
    "                        break\n",
    "        \n",
    "                if not response.ok or len(response.json()) == 0:\n",
    "                    break\n",
    "        \n",
    "                page+=1          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321aca6-0c87-43f7-b887-60b85666bd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7843a0f6-914f-417f-9733-179d3787498f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_body': '              ****\\r\\n'\n",
      "          '\\r\\n'\n",
      "          '_Originally posted by @sarbazvatanatan in '\n",
      "          'https://github.com/openai/openai-cookbook/issues/1103#issuecomment-2063022189_\\r\\n'\n",
      "          '            ',\n",
      " '_closedAt': '2024-04-18T05:22:50Z',\n",
      " '_createdAt': '2024-04-18T05:21:58Z',\n",
      " '_issueNumber': '1159',\n",
      " '_repo': 'openai-cookbook',\n",
      " '_state': 'closed',\n",
      " '_title': '****',\n",
      " '_type': 'issue'}\n"
     ]
    }
   ],
   "source": [
    "#Sample Issue\n",
    "pprint(issues[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449551ce-f1fd-4c1c-83d3-e4d24fa6ba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e6c3e7d-032d-414c-88af-db7d82cd4be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994\n"
     ]
    }
   ],
   "source": [
    "#Number of Issues in the given timeframe\n",
    "pprint(len(issues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c232e5-aa18-4187-b838-3a3b1732ff35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68599a46-1c6b-4897-a4e6-190059999f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of Issues to a DataFrame\n",
    "df_Issues = pd.DataFrame(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ecc4d-cf70-410f-bf16-68bb298a39ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a0a26fa-5fce-4e10-b2b1-5011a43d733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing all NaN values with None in columns as elasticsearch does not recognize it\n",
    "df_Issues.fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76669868-930c-4af6-a8c9-c43b3f2f7af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0ce3ebd-bbd5-4edc-a5ab-ca1ef4c6779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embeddings from OpenAI API\n",
    "def embed(texts):\n",
    "    # Make a request to OpenAI API to get embeddings\n",
    "    embeddings = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model='text-embedding-ada-002'\n",
    "    )\n",
    "    # Extract embeddings from the API response\n",
    "    return [result.embedding for result in embeddings.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a83e0c9-0b7b-48fa-a1a2-6eea57961218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89df7bc0-1fd5-485b-a9c2-29283dfe7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1994 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▊                                                           | 500/1994 [01:03<03:10,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████                                       | 1000/1994 [02:05<02:04,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding batch...\n",
      "Waiting for 1 minute before the next batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1994/1994 [03:07<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494\n"
     ]
    }
   ],
   "source": [
    "## Embedding creation using openAI of GitHub Issues.\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=\"sk-proj-qvAcF7Mh208m9ZGhebNUT3BlbkFJIAbrvFPh70bYnHv57Zp0\")\n",
    "\n",
    "Issue_embeddings = []\n",
    "\n",
    "# Batch size for processing data\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize data structure for storing text\n",
    "data = [\n",
    "    [], # Titles\n",
    "]\n",
    "count=0;\n",
    "# Embed and insert in batches\n",
    "for i in tqdm(range(0, len(df_Issues))):\n",
    "    title = str(df_Issues.iloc[i]['_title']).replace(\"\\n\", \"\") or ''\n",
    "    body = str(df_Issues.iloc[i]['_body']).replace(\"\\n\", \"\") or ''\n",
    "    \n",
    "    # Merge 'repository name','title' and 'body' of the GitHub Issue\n",
    "    combined_text = f\"Repository:{owner}/{repo} Issue Title:{title} Issue Body:{body}\"  \n",
    "    data[0].append(combined_text)\n",
    "    if len(data[0]) % batch_size == 0:\n",
    "        print(\"Embedding batch...\")\n",
    "\n",
    "        embeddings_batch = embed(data[0]) \n",
    "        Issue_embeddings.extend(embeddings_batch)\n",
    "        data = [[]]\n",
    "        print(\"Waiting for 1 minute before the next batch...\")\n",
    "        \n",
    "        time.sleep(60)    \n",
    "        \n",
    "# Embed the remaining data if any\n",
    "if len(data[0]) != 0:\n",
    "    embeddings_rem = embed(data[0])\n",
    "    print(len(embeddings_rem))\n",
    "    Issue_embeddings.extend(embeddings_rem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dacbf-ef0a-4f7c-b13f-427c37113eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a630d2f2-346a-4927-8bd6-761b097f09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Generated embeddings to GitHub_Issue_vector column in the dataframe\n",
    "\n",
    "df_Issues[\"GitHub_Issue_vector\"] = Issue_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca323fed-13c1-4636-bebb-dd7c3257e5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3c634e-a209-4f00-9ed5-59bc16ad45df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>_repo</th>\n",
       "      <th>_issueNumber</th>\n",
       "      <th>_title</th>\n",
       "      <th>_createdAt</th>\n",
       "      <th>_closedAt</th>\n",
       "      <th>_state</th>\n",
       "      <th>_body</th>\n",
       "      <th>GitHub_Issue_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>105643</td>\n",
       "      <td>[8.13] Expand docs about max-shards-per-node (...</td>\n",
       "      <td>2024-02-20T08:44:26Z</td>\n",
       "      <td>2024-02-20T09:01:17Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>Backports the following commits to 8.13:\\n - E...</td>\n",
       "      <td>[-0.0031834025867283344, 0.008741861209273338,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>105642</td>\n",
       "      <td>[8.12] Expand docs about max-shards-per-node (...</td>\n",
       "      <td>2024-02-20T08:44:18Z</td>\n",
       "      <td>2024-02-20T09:03:23Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>Backports the following commits to 8.12:\\n - E...</td>\n",
       "      <td>[-0.0005375546752475202, 0.01213794108480215, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>105641</td>\n",
       "      <td>Reintroduce non-semantic version in NodeInfo (...</td>\n",
       "      <td>2024-02-20T08:41:22Z</td>\n",
       "      <td>2024-12-31T00:36:30Z</td>\n",
       "      <td>open</td>\n",
       "      <td>Re-introduce the change in https://github.com/...</td>\n",
       "      <td>[0.0060468451119959354, 0.0013325520558282733,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>105640</td>\n",
       "      <td>Remove unused field</td>\n",
       "      <td>2024-02-20T08:29:37Z</td>\n",
       "      <td>2024-02-21T07:30:45Z</td>\n",
       "      <td>closed</td>\n",
       "      <td>This change removes a field in TransportPutShu...</td>\n",
       "      <td>[0.010515150614082813, 0.0009875621180981398, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>issue</td>\n",
       "      <td>elasticsearch</td>\n",
       "      <td>105639</td>\n",
       "      <td>[8.13] [DOCS] [Remote clusters] Reference spec...</td>\n",
       "      <td>2024-02-20T08:28:30Z</td>\n",
       "      <td>2024-02-20T09:37:46Z</td>\n",
       "      <td>closed</td>\n",
       "      <td># Backport\\n\\nThis will backport the following...</td>\n",
       "      <td>[-0.01938268356025219, 0.002542417496442795, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _type          _repo _issueNumber  \\\n",
       "1989  issue  elasticsearch       105643   \n",
       "1990  issue  elasticsearch       105642   \n",
       "1991  issue  elasticsearch       105641   \n",
       "1992  issue  elasticsearch       105640   \n",
       "1993  issue  elasticsearch       105639   \n",
       "\n",
       "                                                 _title            _createdAt  \\\n",
       "1989  [8.13] Expand docs about max-shards-per-node (...  2024-02-20T08:44:26Z   \n",
       "1990  [8.12] Expand docs about max-shards-per-node (...  2024-02-20T08:44:18Z   \n",
       "1991  Reintroduce non-semantic version in NodeInfo (...  2024-02-20T08:41:22Z   \n",
       "1992                                Remove unused field  2024-02-20T08:29:37Z   \n",
       "1993  [8.13] [DOCS] [Remote clusters] Reference spec...  2024-02-20T08:28:30Z   \n",
       "\n",
       "                 _closedAt  _state  \\\n",
       "1989  2024-02-20T09:01:17Z  closed   \n",
       "1990  2024-02-20T09:03:23Z  closed   \n",
       "1991  2024-12-31T00:36:30Z    open   \n",
       "1992  2024-02-21T07:30:45Z  closed   \n",
       "1993  2024-02-20T09:37:46Z  closed   \n",
       "\n",
       "                                                  _body  \\\n",
       "1989  Backports the following commits to 8.13:\\n - E...   \n",
       "1990  Backports the following commits to 8.12:\\n - E...   \n",
       "1991  Re-introduce the change in https://github.com/...   \n",
       "1992  This change removes a field in TransportPutShu...   \n",
       "1993  # Backport\\n\\nThis will backport the following...   \n",
       "\n",
       "                                    GitHub_Issue_vector  \n",
       "1989  [-0.0031834025867283344, 0.008741861209273338,...  \n",
       "1990  [-0.0005375546752475202, 0.01213794108480215, ...  \n",
       "1991  [0.0060468451119959354, 0.0013325520558282733,...  \n",
       "1992  [0.010515150614082813, 0.0009875621180981398, ...  \n",
       "1993  [-0.01938268356025219, 0.002542417496442795, 0...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the new Column is created\n",
    "df_Issues.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682465d1-0413-4e65-8f0d-62a1579bb3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95e897aa-f073-4e35-9c2f-645ae5fdee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Elasticsearch connection\n",
    "from elasticsearch import Elasticsearch,helpers\n",
    "es = Elasticsearch(['http://localhost:9200'])\n",
    "es.ping()   #connection testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8622726-a3cf-4a62-a0d9-8d1d57511de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2118a2e8-5c50-49b0-aec5-882a76f606e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'github_issues'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Index Mapping for githubissues\n",
    "\n",
    "index_mapping= {\n",
    "    \"properties\": {\n",
    "      \"GitHub_Issue_vector\": {\n",
    "          \"type\": \"dense_vector\",\n",
    "          \"dims\": 1536,\n",
    "          \"index\": \"true\",\n",
    "          \"similarity\": \"cosine\"\n",
    "      },\n",
    "     \"_type\": {\"type\": \"text\"}, \n",
    "     \"_repo\":{\"type\":\"text\"},   \n",
    "     \"_issueNumber\": {\"type\": \"long\"},    \n",
    "     \"_title\": {\"type\": \"text\"},\n",
    "     \"_createdAt\": {\"type\": \"date\"},\n",
    "     \"_closedAt\": {\"type\": \"date\"},\n",
    "     \"_state\": {\"type\": \"text\"},\n",
    "     \"_body\": {\"type\": \"text\"}\n",
    "   }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=\"github_issues\"):\n",
    "    es.indices.delete(index=\"github_issues\")\n",
    "\n",
    "es.indices.create(index=\"github_issues\", body={\"mappings\": index_mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dca088-74bc-4d7f-914f-a1d6d382a86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9aaa748c-0660-46ae-a0cc-9571ba1c5b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1994 records into Elasticsearch. Failed records: []\n"
     ]
    }
   ],
   "source": [
    "# Bulk indexing for githubissues\n",
    "\n",
    "def dataframe_to_bulk_actions(df_Issues):\n",
    "    for index, row in df_Issues.iterrows():\n",
    "        yield {\n",
    "            \"_index\": 'github_issues',\n",
    "            \"_source\": {\n",
    "                \"_type\": row['_type'],\n",
    "                \"_repo\":row['_repo'],\n",
    "                \"_issueNumber\": row['_issueNumber'],\n",
    "                \"_title\": row['_title'],\n",
    "                \"_createdAt\": row['_createdAt'],\n",
    "                \"_closedAt\": row['_closedAt'],\n",
    "                \"_state\": row['_state'],\n",
    "                \"_body\": row['_body'],\n",
    "                \"GitHub_Issue_vector\": row['GitHub_Issue_vector']\n",
    "            }\n",
    "        }\n",
    "\n",
    "start = 0\n",
    "end = len(df_Issues)\n",
    "batch_size = 500\n",
    "\n",
    "for batch_start in range(start, end, batch_size):\n",
    "    batch_end = min(batch_start + batch_size, end)\n",
    "    batch_dataframe = df_Issues.iloc[batch_start:batch_end]\n",
    "    actions = list(dataframe_to_bulk_actions(df_Issues.iloc[start:end]))\n",
    "    \n",
    "success, failed = helpers.bulk(es, actions)\n",
    "print(f\"Inserted {success} records into Elasticsearch. Failed records: {failed}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac75e1-947c-40c7-bc53-d6159b9becaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
